{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 375, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 112500)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                3600032   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 6,600,645\n",
      "Trainable params: 6,600,645\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 1725 samples, validate on 500 samples\n",
      "Epoch 1/20\n",
      "1725/1725 [==============================] - 8s 4ms/step - loss: 1.6083 - acc: 0.2249 - val_loss: 1.6082 - val_acc: 0.1960\n",
      "Epoch 2/20\n",
      "1725/1725 [==============================] - 6s 4ms/step - loss: 1.6064 - acc: 0.2394 - val_loss: 1.6076 - val_acc: 0.1960\n",
      "Epoch 3/20\n",
      "1725/1725 [==============================] - 6s 4ms/step - loss: 1.6050 - acc: 0.2394 - val_loss: 1.6073 - val_acc: 0.1960\n",
      "Epoch 4/20\n",
      "1725/1725 [==============================] - 6s 3ms/step - loss: 1.6039 - acc: 0.2394 - val_loss: 1.6072 - val_acc: 0.1960\n",
      "Epoch 5/20\n",
      "1725/1725 [==============================] - 6s 4ms/step - loss: 1.6030 - acc: 0.2394 - val_loss: 1.6072 - val_acc: 0.1960\n",
      "Epoch 6/20\n",
      "1725/1725 [==============================] - 7s 4ms/step - loss: 1.6024 - acc: 0.2394 - val_loss: 1.6073 - val_acc: 0.1960\n",
      "Epoch 7/20\n",
      "1725/1725 [==============================] - 6s 4ms/step - loss: 1.6019 - acc: 0.2394 - val_loss: 1.6074 - val_acc: 0.1960\n",
      "Epoch 8/20\n",
      "1725/1725 [==============================] - 6s 3ms/step - loss: 1.6015 - acc: 0.2394 - val_loss: 1.6076 - val_acc: 0.1960\n",
      "Epoch 9/20\n",
      "1725/1725 [==============================] - 6s 3ms/step - loss: 1.6012 - acc: 0.2394 - val_loss: 1.6077 - val_acc: 0.1960\n",
      "Epoch 10/20\n",
      "1725/1725 [==============================] - 6s 3ms/step - loss: 1.6011 - acc: 0.2394 - val_loss: 1.6079 - val_acc: 0.1960\n",
      "Epoch 11/20\n",
      "1725/1725 [==============================] - 7s 4ms/step - loss: 1.6009 - acc: 0.2394 - val_loss: 1.6081 - val_acc: 0.1960\n",
      "Epoch 12/20\n",
      "1725/1725 [==============================] - 6s 3ms/step - loss: 1.6008 - acc: 0.2394 - val_loss: 1.6083 - val_acc: 0.1960\n",
      "Epoch 13/20\n",
      "1725/1725 [==============================] - 6s 3ms/step - loss: 1.6007 - acc: 0.2394 - val_loss: 1.6085 - val_acc: 0.1960\n",
      "Epoch 14/20\n",
      "1725/1725 [==============================] - 6s 3ms/step - loss: 1.6006 - acc: 0.2394 - val_loss: 1.6085 - val_acc: 0.1960\n",
      "Epoch 15/20\n",
      "1725/1725 [==============================] - 6s 3ms/step - loss: 1.6006 - acc: 0.2394 - val_loss: 1.6086 - val_acc: 0.1960\n",
      "Epoch 16/20\n",
      "1725/1725 [==============================] - 6s 3ms/step - loss: 1.6006 - acc: 0.2394 - val_loss: 1.6087 - val_acc: 0.1960\n",
      "Epoch 17/20\n",
      "1725/1725 [==============================] - 6s 3ms/step - loss: 1.6005 - acc: 0.2394 - val_loss: 1.6088 - val_acc: 0.1960\n",
      "Epoch 18/20\n",
      "1725/1725 [==============================] - 6s 4ms/step - loss: 1.6005 - acc: 0.2394 - val_loss: 1.6090 - val_acc: 0.1960\n",
      "Epoch 19/20\n",
      "1725/1725 [==============================] - 6s 4ms/step - loss: 1.6005 - acc: 0.2394 - val_loss: 1.6090 - val_acc: 0.1960\n",
      "Epoch 20/20\n",
      "1725/1725 [==============================] - 6s 3ms/step - loss: 1.6005 - acc: 0.2394 - val_loss: 1.6090 - val_acc: 0.1960\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'D:/Deeplearning/datasets/bbc'\n",
    "labels = []\n",
    "texts = []\n",
    "label_count = 0\n",
    "for label_type in ['business', 'entertainment', 'politics', 'sport', 'tech']:\n",
    "    dir_name = os.path.join(data_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        f = open(os.path.join(dir_name, fname), encoding=\"utf8\", errors='ignore')\n",
    "        texts.append(f.read())\n",
    "        f.close()\n",
    "        labels.append(label_count)\n",
    "    label_count = label_count + 1\n",
    "\n",
    "maxlen = 375 # Cut off after 375 words in tokenizer\n",
    "training_samples = 1725\n",
    "validation_samples = 500\n",
    "max_words = 10000 # Size of dictionary for our problem\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "# Randomly get training and validation samples\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "\n",
    "# Get pre-trained embedding vectors\n",
    "# Each vector has a size of 300\n",
    "glove_dir = 'D:/Deeplearning/datasets/bbc/glove'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "# Embedding dimension is the same as our embedding vector size\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Start creating the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "# Set weights of the embedding layer from our pretrained embedding matrix\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "# Compile and start training for 20 epochs\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_val, y_val), shuffle=True)\n",
    "model.save_weights('bbc_news_classfication_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
